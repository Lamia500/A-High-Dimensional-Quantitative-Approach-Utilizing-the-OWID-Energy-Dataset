{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from __future__ import annotations\n\nimport argparse\nimport json\nimport math\nimport os\nimport warnings\nfrom dataclasses import dataclass\nfrom typing import Dict, List, Optional, Tuple\n\nimport joblib\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.base import BaseEstimator, clone\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import (\n    r2_score,\n    mean_absolute_error,\n    mean_squared_error,\n    median_absolute_error,\n    mean_squared_log_error,\n)\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Models\nfrom sklearn.linear_model import (\n    LinearRegression,\n    Ridge,\n    Lasso,\n    ElasticNet,\n    BayesianRidge,\n    LassoLars,\n    HuberRegressor,\n    OrthogonalMatchingPursuit,\n)\nfrom sklearn.svm import SVR\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import (\n    RandomForestRegressor,\n    ExtraTreesRegressor,\n    GradientBoostingRegressor,\n    AdaBoostRegressor,\n    BaggingRegressor,\n    VotingRegressor,\n    StackingRegressor,\n)\nfrom sklearn.linear_model import RidgeCV\n\n\n# -----------------------------\n# Defaults (you can override via CLI)\n# -----------------------------\n\nDEFAULT_FEATURES = [\n    \"year\",\n    \"population\",\n    \"gdp\",\n    \"biofuel_consumption\",\n    \"coal_consumption\",\n    \"fossil_fuel_consumption\",\n    \"gas_consumption\",\n    \"hydro_consumption\",\n    \"low_carbon_consumption\",\n    \"nuclear_consumption\",\n    \"oil_consumption\",\n    \"other_renewable_consumption\",\n    \"primary_energy_consumption\",\n    \"renewables_consumption\",\n    \"solar_consumption\",\n    \"wind_consumption\",\n]\nENGINEERED_FEATURES = [\"gdp_per_capita\", \"energy_per_capita\"]\n\nDEFAULT_TARGET = \"electricity_generation\"\nDEFAULT_TIME_COL = \"year\"\nDEFAULT_GROUP_CANDIDATES = [\"country\", \"iso_code\"]\nDEFAULT_MISSINGNESS_DROP_THRESHOLD = 0.60\n\n# For RMSLE safety:\nCLIP_PRED_FOR_RMSLE_AT = 0.0\n\n\n# -----------------------------\n# Metrics\n# -----------------------------\n\ndef safe_mape(y_true: np.ndarray, y_pred: np.ndarray, eps: float = 1e-12) -> float:\n    denom = np.maximum(np.abs(y_true), eps)\n    return float(np.mean(np.abs((y_true - y_pred) / denom)) * 100.0)\n\ndef safe_rmsle(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n    y_true_c = np.maximum(y_true, 0.0)\n    y_pred_c = np.maximum(y_pred, CLIP_PRED_FOR_RMSLE_AT)\n    return float(math.sqrt(mean_squared_log_error(y_true_c, y_pred_c)))\n\ndef regression_report(y_true: np.ndarray, y_pred: np.ndarray) -> Dict[str, float]:\n    return {\n        \"R2\": float(r2_score(y_true, y_pred)),\n        \"MAE\": float(mean_absolute_error(y_true, y_pred)),\n        \"RMSE\": float(math.sqrt(mean_squared_error(y_true, y_pred))),\n        \"MedAE\": float(median_absolute_error(y_true, y_pred)),\n        \"MAPE\": float(safe_mape(y_true, y_pred)),\n        \"RMSLE\": float(safe_rmsle(y_true, y_pred)),\n    }\n\n\n# -----------------------------\n# Data helpers\n# -----------------------------\n\ndef load_csv(path: str) -> pd.DataFrame:\n    if not os.path.exists(path):\n        raise FileNotFoundError(f\"CSV not found: {path}\")\n    df = pd.read_csv(path)\n    df.columns = [c.strip() for c in df.columns]\n    return df\n\ndef drop_high_missingness(df: pd.DataFrame, keep_cols: List[str], threshold: float) -> pd.DataFrame:\n    miss_frac = df.isna().mean()\n    drop_cols = [c for c in df.columns if (miss_frac.get(c, 0.0) > threshold and c not in keep_cols)]\n    return df.drop(columns=drop_cols)\n\ndef interpolate_continuous(\n    df: pd.DataFrame,\n    continuous_cols: List[str],\n    group_col: Optional[str],\n    time_col: str,\n) -> pd.DataFrame:\n    df = df.copy()\n\n    if time_col not in df.columns:\n        # If there's no time column, skip interpolation.\n        return df\n\n    sort_cols = [time_col]\n    if group_col and group_col in df.columns:\n        sort_cols = [group_col, time_col]\n\n    df = df.sort_values(sort_cols)\n\n    if group_col and group_col in df.columns:\n        df[continuous_cols] = (\n            df.groupby(group_col, group_keys=False)[continuous_cols]\n              .apply(lambda g: g.interpolate(method=\"linear\", limit_direction=\"both\"))\n        )\n    else:\n        df[continuous_cols] = df[continuous_cols].interpolate(method=\"linear\", limit_direction=\"both\")\n\n    return df\n\ndef impute_categoricals_with_mode(df: pd.DataFrame, cat_cols: List[str]) -> pd.DataFrame:\n    df = df.copy()\n    for c in cat_cols:\n        mode = df[c].mode(dropna=True)\n        fill_val = mode.iloc[0] if len(mode) else \"unknown\"\n        df[c] = df[c].fillna(fill_val)\n    return df\n\ndef feature_engineering(df: pd.DataFrame, population_col: str = \"population\") -> pd.DataFrame:\n    df = df.copy()\n    if \"gdp\" in df.columns and population_col in df.columns:\n        pop = df[population_col].replace(0, np.nan)\n        df[\"gdp_per_capita\"] = df[\"gdp\"] / pop\n\n    if \"primary_energy_consumption\" in df.columns and population_col in df.columns:\n        pop = df[population_col].replace(0, np.nan)\n        df[\"energy_per_capita\"] = df[\"primary_energy_consumption\"] / pop\n\n    return df\n\ndef pick_group_col(df: pd.DataFrame, user_group_col: Optional[str]) -> Optional[str]:\n    if user_group_col and user_group_col in df.columns:\n        return user_group_col\n    for c in DEFAULT_GROUP_CANDIDATES:\n        if c in df.columns:\n            return c\n    return None\n\ndef prepare_xy(\n    df: pd.DataFrame,\n    target: str,\n    features: Optional[List[str]],\n) -> Tuple[pd.DataFrame, pd.Series, List[str]]:\n    if target not in df.columns:\n        cols_preview = \", \".join(list(df.columns)[:40])\n        raise ValueError(\n            f\"Target column '{target}' not found in your CSV.\\n\"\n            f\"Columns (first ~40): {cols_preview}\\n\"\n            f\"Tip: run with --target YOUR_COLUMN_NAME\"\n        )\n\n    if features is None:\n        feature_candidates = DEFAULT_FEATURES + ENGINEERED_FEATURES\n    else:\n        feature_candidates = features + ENGINEERED_FEATURES  # still allow engineered if possible\n\n    present_features = [c for c in feature_candidates if c in df.columns]\n    if len(present_features) == 0:\n        cols_preview = \", \".join(list(df.columns)[:40])\n        raise ValueError(\n            \"No requested feature columns were found in your CSV.\\n\"\n            f\"Columns (first ~40): {cols_preview}\\n\"\n            \"Tip: run with --features col1,col2,col3\"\n        )\n\n    X = df[present_features].copy()\n    y = df[target].copy()\n    return X, y, present_features\n\n\n# -----------------------------\n# Modeling\n# -----------------------------\n\ndef build_preprocess_pipeline(numeric_features: List[str]) -> ColumnTransformer:\n    num_pipe = Pipeline(\n        steps=[\n            (\"imputer\", SimpleImputer(strategy=\"median\")),\n            (\"scaler\", MinMaxScaler()),\n        ]\n    )\n    return ColumnTransformer(\n        transformers=[(\"num\", num_pipe, numeric_features)],\n        remainder=\"drop\",\n        verbose_feature_names_out=False,\n    )\n\ndef make_model_catalog(random_state: int) -> Dict[str, BaseEstimator]:\n    return {\n        \"LinearRegression\": LinearRegression(),\n        \"Ridge\": Ridge(random_state=random_state),\n        \"Lasso\": Lasso(random_state=random_state, max_iter=20000),\n        \"ElasticNet\": ElasticNet(random_state=random_state, max_iter=20000),\n        \"BayesianRidge\": BayesianRidge(),\n        \"OMP\": OrthogonalMatchingPursuit(),\n        \"LassoLars\": LassoLars(),\n        \"HuberRegressor\": HuberRegressor(),\n        \"DecisionTree\": DecisionTreeRegressor(random_state=random_state),\n        \"RandomForest\": RandomForestRegressor(random_state=random_state, n_jobs=-1),\n        \"ExtraTrees\": ExtraTreesRegressor(random_state=random_state, n_jobs=-1),\n        \"GradientBoosting\": GradientBoostingRegressor(random_state=random_state),\n        \"AdaBoost\": AdaBoostRegressor(random_state=random_state),\n        \"KNN\": KNeighborsRegressor(),\n        \"SVR\": SVR(),\n        \"VotingRegressor\": VotingRegressor(\n            estimators=[\n                (\"rf\", RandomForestRegressor(random_state=random_state, n_estimators=300, n_jobs=-1)),\n                (\"et\", ExtraTreesRegressor(random_state=random_state, n_estimators=300, n_jobs=-1)),\n                (\"ridge\", Ridge(random_state=random_state)),\n            ]\n        ),\n        \"StackingRegressor\": StackingRegressor(\n            estimators=[\n                (\"rf\", RandomForestRegressor(random_state=random_state, n_estimators=300, n_jobs=-1)),\n                (\"et\", ExtraTreesRegressor(random_state=random_state, n_estimators=300, n_jobs=-1)),\n            ],\n            final_estimator=RidgeCV(),\n            passthrough=False,\n            n_jobs=-1,\n        ),\n        \"BaggingRegressor\": BaggingRegressor(\n            estimator=DecisionTreeRegressor(random_state=random_state),\n            random_state=random_state,\n            n_jobs=-1,\n        ),\n    }\n\ndef evaluate_models(\n    X_train: pd.DataFrame,\n    y_train: pd.Series,\n    X_test: pd.DataFrame,\n    y_test: pd.Series,\n    models: Dict[str, BaseEstimator],\n    preprocess: ColumnTransformer,\n) -> pd.DataFrame:\n    rows = []\n    for name, model in models.items():\n        pipe = Pipeline(steps=[(\"pre\", preprocess), (\"model\", clone(model))])\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")\n            pipe.fit(X_train, y_train)\n\n        y_pred = pipe.predict(X_test)\n        metrics = regression_report(y_test.to_numpy(), np.asarray(y_pred))\n        metrics[\"Model\"] = name\n        rows.append(metrics)\n\n    return pd.DataFrame(rows).set_index(\"Model\").sort_values(\"R2\", ascending=False)\n\ndef make_param_grids() -> Dict[str, List[Dict[str, object]]]:\n    return {\n        \"RandomForest\": [\n            {\n                \"model__n_estimators\": [200, 300, 400, 500],\n                \"model__max_depth\": [None, 10, 20, 30, 40],\n                \"model__min_samples_split\": [2, 5, 10],\n                \"model__min_samples_leaf\": [1, 2, 4],\n                \"model__max_features\": [\"sqrt\", \"log2\", 0.5, 1.0],\n            }\n        ],\n        \"ExtraTrees\": [\n            {\n                \"model__n_estimators\": [200, 300, 400],\n                \"model__max_depth\": [None, 10, 20, 30],\n                \"model__min_samples_split\": [2, 5, 10],\n                \"model__min_samples_leaf\": [1, 2, 4],\n                \"model__max_features\": [\"sqrt\", \"log2\", 0.5],\n            }\n        ],\n        \"GradientBoosting\": [\n            {\n                \"model__n_estimators\": [100, 200, 300],\n                \"model__learning_rate\": [0.01, 0.05, 0.1, 0.2],\n                \"model__max_depth\": [3, 4, 5],\n                \"model__subsample\": [0.7, 0.8, 1.0],\n                \"model__min_samples_split\": [2, 5, 10],\n                \"model__n_iter_no_change\": [5, 10],\n                \"model__tol\": [1e-3, 1e-4],\n            }\n        ],\n        \"BaggingRegressor\": [\n            {\n                \"model__n_estimators\": [50, 100, 200],\n                \"model__max_samples\": [0.5, 0.7, 1.0],\n                \"model__max_features\": [0.5, 0.7, 1.0],\n            }\n        ],\n        \"Ridge\": [{\"model__alpha\": [0.1, 1.0, 10.0]}],\n        \"Lasso\": [{\"model__alpha\": [0.1, 1.0, 10.0]}],\n        \"ElasticNet\": [{\"model__alpha\": [0.1, 1.0, 10.0], \"model__l1_ratio\": [0.1, 0.5, 0.9]}],\n        \"BayesianRidge\": [\n            {\n                \"model__alpha_1\": [1e-6, 1e-3, 1.0],\n                \"model__alpha_2\": [1e-6, 1e-3, 1.0],\n                \"model__lambda_1\": [1e-6, 1e-3, 1.0],\n                \"model__lambda_2\": [1e-6, 1e-3, 1.0],\n            }\n        ],\n        \"OMP\": [{\"model__n_nonzero_coefs\": [5, 10, 20], \"model__max_iter\": [100, 500, 1000]}],\n        \"LassoLars\": [{\"model__alpha\": [0.1, 1.0, 10.0]}],\n        \"HuberRegressor\": [{\"model__alpha\": [0.1, 1.0, 10.0]}],\n        \"KNN\": [\n            {\n                \"model__n_neighbors\": [3, 5, 7, 10, 15, 20],\n                \"model__weights\": [\"uniform\", \"distance\"],\n                \"model__metric\": [\"minkowski\", \"euclidean\", \"manhattan\", \"chebyshev\"],\n                \"model__p\": [1, 2],\n                \"model__algorithm\": [\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"],\n            },\n            {\n                \"model__n_neighbors\": [3, 5, 7, 10, 15, 20],\n                \"model__weights\": [\"uniform\", \"distance\"],\n                \"model__metric\": [\"cosine\"],\n                \"model__algorithm\": [\"brute\"],\n            },\n        ],\n    }\n\ndef tune_one(\n    model_name: str,\n    model: BaseEstimator,\n    preprocess: ColumnTransformer,\n    X_train: pd.DataFrame,\n    y_train: pd.Series,\n    grid: List[Dict[str, object]],\n    cv: int,\n    scoring: str,\n) -> GridSearchCV:\n    pipe = Pipeline(steps=[(\"pre\", preprocess), (\"model\", clone(model))])\n    gs = GridSearchCV(\n        estimator=pipe,\n        param_grid=grid,\n        cv=cv,\n        scoring=scoring,\n        n_jobs=-1,\n        verbose=0,\n    )\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        gs.fit(X_train, y_train)\n    return gs\n\n\n# -----------------------------\n# Main\n# -----------------------------\n\ndef parse_args() -> argparse.Namespace:\n    p = argparse.ArgumentParser(description=\"Local PC regression pipeline for energy/electricity generation prediction.\")\n    p.add_argument(\"--data\", required=True, help=\"Path to your CSV dataset.\")\n    p.add_argument(\"--target\", default=DEFAULT_TARGET, help=f\"Target column name (default: {DEFAULT_TARGET}).\")\n    p.add_argument(\"--time-col\", default=DEFAULT_TIME_COL, help=f\"Time column for interpolation (default: {DEFAULT_TIME_COL}).\")\n    p.add_argument(\"--group-col\", default=None, help=\"Optional group column for within-group interpolation (e.g., country).\")\n    p.add_argument(\"--features\", default=None, help=\"Comma-separated list of feature columns. If omitted, uses paper-like defaults.\")\n    p.add_argument(\"--test-size\", type=float, default=0.30, help=\"Test size fraction (default: 0.30).\")\n    p.add_argument(\"--random-state\", type=int, default=42, help=\"Random seed (default: 42).\")\n    p.add_argument(\"--missing-drop-threshold\", type=float, default=DEFAULT_MISSINGNESS_DROP_THRESHOLD,\n                   help=\"Drop columns with missing fraction above this (default: 0.60).\")\n    p.add_argument(\"--no-tune\", action=\"store_true\", help=\"Skip GridSearchCV tuning (faster).\")\n    p.add_argument(\"--cv\", type=int, default=5, help=\"CV folds for GridSearchCV (default: 5).\")\n    p.add_argument(\"--scoring\", default=\"r2\", help=\"Scoring metric for tuning (default: r2).\")\n    p.add_argument(\"--output-dir\", default=\"output\", help=\"Where to save results/models (default: output/).\")\n    return p.parse_args()\n\ndef main() -> None:\n    args = parse_args()\n    os.makedirs(args.output_dir, exist_ok=True)\n\n    df = load_csv(args.data)\n\n    # Feature list (optional override)\n    user_features = None\n    if args.features:\n        user_features = [c.strip() for c in args.features.split(\",\") if c.strip()]\n\n    group_col = pick_group_col(df, args.group_col)\n\n    # Keep columns required for core work (target + time + group)\n    keep_cols = set((user_features or DEFAULT_FEATURES) + [args.target, args.time_col])\n    if group_col:\n        keep_cols.add(group_col)\n\n    # Feature engineering before dropping columns (so engineered cols can be made)\n    df = feature_engineering(df)\n\n    # Drop high-missingness columns (except keep_cols)\n    df = drop_high_missingness(df, keep_cols=list(keep_cols), threshold=args.missing_drop_threshold)\n\n    # Identify categorical/numeric\n    cat_cols = [c for c in df.columns if df[c].dtype == \"object\"]\n    numeric_cols = [c for c in df.columns if c not in cat_cols]\n\n    # Interpolate numeric features (exclude target to avoid smoothing y)\n    cont_cols = [c for c in numeric_cols if c != args.target]\n    df = interpolate_continuous(df, continuous_cols=cont_cols, group_col=group_col, time_col=args.time_col)\n\n    # Mode-impute categoricals\n    if cat_cols:\n        df = impute_categoricals_with_mode(df, cat_cols)\n\n    # Drop rows with missing target\n    df = df.dropna(subset=[args.target]).reset_index(drop=True)\n\n    # Prepare X/y\n    X, y, used_features = prepare_xy(df, target=args.target, features=user_features)\n\n    # Train/test split\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y,\n        test_size=args.test_size,\n        random_state=args.random_state,\n    )\n\n    preprocess = build_preprocess_pipeline(numeric_features=list(X.columns))\n    models = make_model_catalog(random_state=args.random_state)\n\n    # Baseline evaluation\n    baseline = evaluate_models(X_train, y_train, X_test, y_test, models, preprocess)\n    baseline_path = os.path.join(args.output_dir, \"baseline_results.csv\")\n    baseline.to_csv(baseline_path)\n\n    print(\"\\n=== Baseline results (sorted by R2) ===\")\n    print(baseline.round(4))\n    print(f\"\\nSaved: {baseline_path}\")\n\n    # Optionally tune\n    if args.no_tune:\n        print(\"\\nSkipping tuning because --no-tune was set.\")\n        return\n\n    grids = make_param_grids()\n    tuned_rows = []\n    best_overall = None  # (model_name, best_estimator, test_metrics, best_params)\n\n    for model_name, grid in grids.items():\n        if model_name not in models:\n            continue\n\n        print(f\"\\nTuning: {model_name} ...\")\n        gs = tune_one(\n            model_name=model_name,\n            model=models[model_name],\n            preprocess=preprocess,\n            X_train=X_train,\n            y_train=y_train,\n            grid=grid,\n            cv=args.cv,\n            scoring=args.scoring,\n        )\n\n        best_pipe = gs.best_estimator_\n        y_pred = best_pipe.predict(X_test)\n        metrics = regression_report(y_test.to_numpy(), np.asarray(y_pred))\n\n        row = {\"Model\": model_name, **metrics, \"BestParams\": json.dumps(gs.best_params_)}\n        tuned_rows.append(row)\n\n        print(f\"  Best CV {args.scoring}: {gs.best_score_:.6f}\")\n        print(f\"  Test R2: {metrics['R2']:.6f} | RMSE: {metrics['RMSE']:.4f}\")\n\n        if best_overall is None or metrics[\"R2\"] > best_overall[2][\"R2\"]:\n            best_overall = (model_name, best_pipe, metrics, gs.best_params_)\n\n    tuned = pd.DataFrame(tuned_rows).set_index(\"Model\").sort_values(\"R2\", ascending=False)\n    tuned_path = os.path.join(args.output_dir, \"tuned_results.csv\")\n    tuned.to_csv(tuned_path)\n\n    print(\"\\n=== Tuned results (sorted by R2) ===\")\n    print(tuned[[\"R2\", \"MAE\", \"RMSE\", \"MedAE\", \"MAPE\", \"RMSLE\"]].round(4))\n    print(f\"\\nSaved: {tuned_path}\")\n\n    # Save best model\n    if best_overall is not None:\n        best_name, best_pipe, best_metrics, best_params = best_overall\n        model_path = os.path.join(args.output_dir, f\"best_model_{best_name}.joblib\")\n        joblib.dump(\n            {\n                \"model_name\": best_name,\n                \"pipeline\": best_pipe,\n                \"used_features\": used_features,\n                \"target\": args.target,\n                \"test_metrics\": best_metrics,\n                \"best_params\": best_params,\n            },\n            model_path,\n        )\n        print(f\"\\nBest model: {best_name}\")\n        print(f\"Saved best model to: {model_path}\")\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null}]}